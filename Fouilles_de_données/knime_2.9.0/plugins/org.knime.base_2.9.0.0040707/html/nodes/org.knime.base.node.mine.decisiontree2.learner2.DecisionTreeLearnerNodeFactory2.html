<?xml version="1.0" encoding="UTF-8"?>
<html xmlns:t="http://knime.org/node2012" xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>
                    Node description for
                    Decision Tree Learner</title>
<style type="text/css">body {
	background-color: #fff;
	font-family: Tahoma, Arial, Helvetica; 
	font-size: 10pt;
	padding: 0.1em 0.1em 1.5em 0.1em;
}

tt, pre {
	font-family: Courier New, Courier, Arial, Helvetica; 
	font-size: 110%;
}

p {
	text-align: justify;
	/*padding: 0 10px;*/
}



h1{
	font-size: 200%;
	text-align: center;
	border-bottom: thick solid #ffd600;
}

h2{
	font-size: 140%;
	/*padding-left: 5px;*/
	border-bottom: thin dotted #ffd600;
}

.deprecated {
	color: red; font-style: italic;
}

dl{
	margin-left: 1px;
	padding: 0;
}

dt {
	font-weight: bold;
	margin-bottom: 0.3em;
}

dd{
	margin-bottom: 1em;
	margin-left: 1.5em; 
}

table{
	font-size:100%;
}

table.introtable {
    margin-top: 0.5em;
    margin-bottom: 0.5em;
    margin-left: auto;
    margin-right: auto;
    border: 2px solid #666666;
    border-collapse:collapse;
}


table.introtable tr td,th {
    padding: 0.5em 0.5em;
    border: 1px solid #666666;
}


table.introtable tr th {
    background-color: #EEEEEE;
}


td{
	padding: 0.5em 0.5em;
	vertical-align:top;
}


.dt {
	font-weight: bold;
}

a{
	color: #000;
}
	
a:visited{
	color: #666;
}

.warn{
	color: #cf0000;
}

div.group {
    border: 1px solid #ffd600;
    padding: 0.2em;
    margin-bottom: 0.5em;
}

div.groupname {
    text-align: center;
    font-weight: bold;
    background-color: #EEEEEE;
    padding: 0.1em;
}

div#origin-bundle {
    color: #cccccc;
    font-size: 70%;
    margin-top: 2em;
    padding-top: 0.5em;
    border-top: 1px solid #bbbbbb;
}
</style>
</head>
<body>
<h1>Decision Tree Learner</h1>
<p>
			This node induces a classification decision tree in main memory.
			The target attribute must be nominal. The other attributes used for
			decision making can be either nominal or numerical. Numeric splits
			are always binary (two outcomes), dividing the domain in two partitions at a 
			given split point. Nominal splits can be either binary (two outcomes) or 
			they can have as many outcomes as nominal values. In the 
			case of a binary split the nominal values are divided into two subsets.
			The algorithm provides two quality measures for split calculation;
			the gini index and the gain ratio. Further, there exist a
			post pruning method to reduce the tree size and increase prediction
			accuracy. The pruning method is based on the minimum
			description length principle.<br/>
			The algorithm can be run in multiple threads, and thus, exploit multiple
			processors or cores.<br/>
			Most of the techniques used in this decision tree implementation
			can be found in "C4.5 Programs for machine learning", by J.R. Quinlan
			and in "SPRINT: A Scalable Parallel Classifier for Data Mining", by
			J. Shafer, R. Agrawal, M. Mehta (<a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.104.152&amp;rep=rep1&amp;type=pdf">http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.104.152&amp;rep=rep1&amp;type=pdf</a>)<br/>
			If the optional PMML inport is connected and contains 
            preprocessing operations in the TransformationDictionary those are 
            added to the learned model.
		</p>
<h2>Dialog Options</h2>
<dl>
<dt>Class column</dt>
<dd>To select the target attribute. 
			Only nominal attributes are allowed</dd>
<dt>Quality measure</dt>
<dd>
			To select the quality measure according to which the 
			split is calculated. Available are the "Gini Index" and the "Gain Ratio".
		</dd>
<dt>Pruning method</dt>
<dd>
			Pruning reduces tree size and avoids overfitting which increases 
			the generalization performance, and thus, the prediction quality
			(for predictions, use the "Decision Tree Predictor" node).
			Available is the "Minimal Description Length" (MDL) pruning or
			it can also be switched off.
		</dd>
<dt>Reduced Error Pruning</dt>
<dd>
			If checked (default), a simple pruning method is used to cut the tree in a post-processing step: Starting at the leaves, each node is 
			replaced with its most popular class, but only if the prediction accuracy doesn't decrease. Reduced error pruning has the advantage of 
			simplicity and speed.
		</dd>
<dt>Min number records per node</dt>
<dd>
			To select the minimum number of records at least required in each node. If 
			the number of records is smaller or equal to this number the tree
			is not grown any further. This corresponds to a stopping criteria (pre pruning).
		</dd>
<dt>Number records to store for view</dt>
<dd>
			To select the number of records stored in the tree for the view.
			The records are necessary to enable highlighting.
		</dd>
<dt>Average split point</dt>
<dd>
			If checked (default), the split value for numeric attributes is determined according to the mean
			value of the two attribute values that separate the two partitions.
			If unchecked, the split value is set to the largest value of the lower partition (like C4.5).
		</dd>
<dt>Number threads</dt>
<dd>
            This node can exploit multiple threads and thus multiple processors
            or cores. This can improve performance. The default value is set to 
            the number of processors or cores that are available to KNIME. If
            set to 1, the algorithm is performed sequentially.
        </dd>
<dt>Skip nominal columns without domain information</dt>
<dd>
            If checked, nominal columns containing no domain value information are
            skipped. This is generally the case for nominal columns that have
            too many different values.
        </dd>
<dt>Binary nominal splits</dt>
<dd>
			If checked, nominal attributes are split in a binary fashion. Binary
			splits are more difficult to calculate but result also in more 
			accurate trees. The nominal values are divided in two subsets (one
			for each child). If unchecked, for each nominal value one child is
			created. 
		</dd>
<dt>Max #nominal</dt>
<dd>
			The subsets for the binary nominal splits are difficult to calculate.
			To find the best subsets for n nominal values there must be 
			performed 2^n calculations. In case of many different nominal values
			this can be prohibitive expensive. Thus the maximum number of nominal
			values can be defined for which all possible subsets are calculated.
			Above this threshold, a heuristic is applied that first calculates the
			best nominal value for the second partition, then the second best value, 
			and so on; until no improvement can be achieved.
		</dd>
<dt>Filter invalid attribute values in child nodes</dt>
<dd>
		    Binary splits on nominal values may lead to tests for attribute 
		    values, which have been filtered out by a parent tree node.
		    This is due to the fact that the learning algorithm is consistently
		    using the table's domain information instead of the data in a tree
		    node to define the split sets. These duplicate checks do not harm
		    (the tree is the same and and will classify unknown data the exact
		    same way), though they are confusing when the tree is inspected
		    in the tree viewer. Enabling this option will post-process the tree
		    and filter invalid checks.    
		</dd>
<dt>No true child strategy</dt>
<dd>
		If the scoring reaches a node, at which its attributes value is unknown, one of the two following strategies can be used:
		<br/>
		<b>returnNullPrediciton:</b> predict a missing value <br/>
		<b>returnLastPrediction:</b> return the majority class of the last node
		</dd>
<dt>Missing value strategy</dt>
<dd>
		If there are missing values in the data to be predicted, a strategy can be chosen how to handle 
		them:<br/>
			    <b>lastPrediction:</b> use the last known class <br/>
	<!--<b>nullPrediction:</b> predict a missing value<br/>--><!-- not supported -->
		<b>defaultChild:</b> use the default child and continue traversing its path<br/>
	<!--  	<b>weightedConfidence:</b> weight the output of each subtree<br/>--><!-- not supported -->
	<!--  	<b>aggregateNodes:</b> <br/>--><!-- not supported -->
		<b>NONE:</b> use the noTrueChildStrategy
		</dd>
</dl>
<h2>Ports</h2>
<dl>
<div class="group">
<div class="groupname">Input Ports</div>
<table>
<tr>
<td class="dt">0</td>
<td>
				The pre-classified data that should be used to induce the decision tree.
				At least one attribute must be nominal.
		</td>
</tr>
<tr>
<td class="dt">1</td>
<td>Optional PMML port object 
            containing preprocessing operations.</td>
</tr>
</table>
</div>
<div class="group">
<div class="groupname">Output Ports</div>
<table>
<tr>
<td class="dt">0</td>
<td>
			The induced decision tree. The model can be used to classify
			data with unknown target (class) attribute. To do so, connect the 
			model out port to the "Decision Tree Predictor" node.
		</td>
</tr>
</table>
</div>
</dl>
<h2>Views</h2>
<dl>
<dt>Decision Tree View</dt>
<dd>
            Visualizes the learned decision tree. The tree can be expanded and
            collapsed with the plus/minus signs. 		
		</dd>
<dt>Decision Tree View (simple)</dt>
<dd>
            Visualizes the learned decision tree. The tree can be expanded and
            collapsed with the plus/minus signs. The squared brackets show the
            splitting criteria. This is the attribute name on which the parent
            node was split and the value (numeric) and nominal value (set) that
            has led to this child. The class value in single quotes states the
            majority class in this node. The value in round brackets states
            (x of y) where x is the quantity of the majority class and y is the
            total count of examples in this node. The bar with the black border
            and partly filled with yellow color represents the amount
            of records that belongs to the node relatively to the parent node.
            The colored pie chart renders the distribution of the color attribute
            associated with the input data table. NOTE: the colors not necessarily 
            reflect the class attribute. If the color distribution  and the 
            target attribute should correspond to each other, ensure that the 
            "Color Manager" node colors the same attribute as selected in this
            decision tree node as target attribute.
        </dd>
</dl>
<div id="origin-bundle">
                    This node is contained in <em>KNIME Base Nodes</em>
                    provided by <em>KNIME GmbH, Konstanz, Germany</em>.
                </div>
</body>
</html>
